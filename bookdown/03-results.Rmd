# Results

```{r res-setup, include = FALSE}
import::here("data.table", "fread", "uniqueN")
import::here("dplyr", "arrange", "mutate", "recode", "rename", "select")
import::here("here", "here")
import::here("knitr", "include_graphics")
import::here(
  "kableExtra",
  "add_header_above",
  "collapse_rows",
  "kable_minimal",
  "kable_styling",
  "kbl",
  "landscape"
)
import::here("magrittr", "%>%", "extract", "multiply_by", "set_names")
import::here("purrr", "map", "reduce")
import::here("tidyr", "pivot_wider")
import::here("withr", "with_options")

# Suppress chunk output by default.
knitr::opts_chunk$set(echo = FALSE)

here::i_am("bookdown/03-results.Rmd")

analysis_root <- "cancer-cleaning-output"
main_plot_dir <- "main_plots"
main_plot_path <- here(paste(analysis_root, main_plot_dir, sep = "/"))

plot_prefixes <- list(
  "rnd_sim" = "random_deconv_simulation_metric_summary_plot",
  "pbulk_cexpr_acc" = "simulation_true_v_predict_cancer_corr_plot",
  "bp_cexpr_acc" = "bayes_prism_true_v_predict_cancer_corr_plot",
  "pred_cat_perf" = "categorical_prediction_performance_plot",
  "pred_srv_perf" = "survival_prediction_performance_plot"
)

plot_paths <- map(
  plot_prefixes,
  \(prefix) paste(main_plot_path, paste0(prefix, ".png"), sep = "/")
)

plot_caps <- map(
  plot_prefixes,
  \(prefix) {
    paste(main_plot_path, paste0(prefix, "_plot_caption.txt"), sep = "/")
  }
) %>%
  map(readLines)

plot_data_paths <- map(
  plot_prefixes,
  \(prefix) paste(main_plot_path, paste0(prefix, ".csv"), sep = "/")
)

table_caps <- map(
  plot_prefixes,
  \(prefix) {
    paste(main_plot_path, paste0(prefix, "_table_caption.txt"), sep = "/")
  }
) %>%
  map(readLines)

# TODO Add table captions.
```

## Proof of concept

### Normally distributed simulation

<!-- TODO Rephrase this as groups having more technical noise. -->

<!-- TODO Go into more detail of the behavior, especially pairwise cell type
expression correlation. -->

During simulation using normally distributed data, my method's performance
behaved as expected. The estimation accuracy for cancer expression via
deconvolution model residuals (Figure \@ref(fig:rnd-sim-plt), Table
\@ref(tab:rnd-sim-res-tab)) shows clear differences across simulation
parameters, with the largest difference being across reference type.

The average correlation of true and estimated cancer expression under the
homogeneous cancer expression model using a reference with cancer profiles was
close to zero. However, there were still differences between relative levels of
added noise: Absent noise and noise equal to biological variability had the same
levels of accuracy, while the high noise group was slightly worse, and the group
with very high biological noise had exactly zero correlation. Under the model of
cancer cells varying stronger than other cell type's cells, and still including
cancer profiles in the reference, there was an appreciable improvement in
accuracy compared to the homogeneous model. The performance across groups
remained similar to the previous model. As mentioned, the largest improvement
can be seen when removing the cancer expression profile from the reference.
Here, correlation of true and estimated cancer expression is high across almost
all levels of technical noise except for the highest. This extends across cancer
expression models, with the heterogeneous model still showing slightly higher
correlations. Under these optimal conditions, the average correlation ranged
between slightly under 0.85, and slightly under 0.8 for absent and equal
technial noise.

```{r rnd-sim-plt, fig.cap = plot_caps$rnd_sim}
include_graphics(plot_paths$rnd_sim)
```

```{r rnd-sim-res-tab}
rnd_sim_data <- plot_data_paths$rnd_sim %>%
  fread(sep = ",") %>%
  mutate(across(
    where(is.character),
    \(char_col) factor(char_col, levels = unique(char_col))
  ))

# Recode types so they appear in order when sorting cols after pivot_longer.
type_recode <- list(
  # Currently unused.
  "True vs. predicted Cell type abundance 1 / RMSE" = "a ctype_abundances",
  "Cancer expression vs. Model residuals R²" = "b cancer_accuracy"
)

# Depends on the same ordering as type_recode.
type_headers <- c(
  # Currently unused.
  "True vs. predicted Cell type abundance 1 / RMSE" = "Cell type accurracy, $1 / RMSE$",
  "Cancer expression vs. Model residuals R²" = "Cancer expression accurracy $\\rho$"
) %>%
  extract(names(.) %in% unique(rnd_sim_data$type)) %>%
  # Create a list with previous values as names and `2` (fixed width, mean & ci)
  # as values.
  {
    set_names(rep(list(2), length(.)), .)
  }

rnd_sim_data %>%
  # Maybe the cols removed here will be removed somewhere upriver already. If
  # that's the case, this can be removed.
  select(-any_of(c("n", "n_rmse"))) %>%
  mutate(
    `95% CI` = paste("\U00B1", round(mean - min_ci, 3)),
    type = recode(type, !!!type_recode)
  ) %>%
  select(-matches("_ci")) %>%
  pivot_wider(
    values_from = c("mean", "95% CI"),
    names_from = "type",
    names_glue = "{type} {.value}"
  ) %>%
  select(
    heterogeneous_cancer,
    split,
    bio_tech_ratio,
    # Should in ctype acc first and mean first
    all_of(sort(names(.), decr = TRUE))
  ) %>%
  arrange(heterogeneous_cancer, split, bio_tech_ratio) %>%
  kbl(
    caption = table_caps$rnd_sim,
    escape = TRUE,
    col.names = c(
      "Cancer model",
      "Reference type",
      "Relative noise level name",
      rep(c("Average", "95% CI"), uniqueN(rnd_sim_data$type))
    ),
    align = "l"
  ) %>%
  kable_minimal() %>%
  add_header_above(c(" " = 3, type_headers), escape = FALSE) %>%
  collapse_rows(
    columns = 1:3, valign = "top", row_group_label_position = "first"
  ) %>%
  landscape()
```

### scRNA-seq pseudo-bulks

When benchmarking my method on pseudo-bulks derived from scRNA-seq data, the
accuracy of estimated cancer expression from absolute values of model residuals
was high (Figure \@ref(fig:pbulk-cexpr-acc-plt), Table
\@ref(tab:pbulk-cexpr-acc-tab)) with only minor differences in average
correlation across parameters.

When including cancer expression in the reference profiles, there was a clear
difference between marker selection methods. Wilcoxon test based markers showed
higher average correlation compared to random marker selection across all
thresholds on the number of markers (no 95% CI overlap). Also, while there was
very little variation in accuracy across thresholds for random markers, on the
second-tightest threshold for Wilcoxon based markers performance was
significantly worse than the on the remaining thresholds. When excluding the
cancer expression profile from the reference, performance further increased
across all other parameters (no pairwise 95% CI overlap), except for the
tightest marker threshold on Wilcoxon.

General patterns were similar between in- or excluding the cancer expression
profile but with some exceptions. In the latter case and for the loosest marker
threshold, random markers resulted in slightly but significantly worse average
correlation than looser thresholds, which in turn no longer clearly differed
from the second-tightest threshold on Wilcoxon markers. Lastly, the within group
differences for Wilcoxon markers increased such that the two loosest thresholds
no longer resulted in similar performance. The second-loosest marker threshold
now resulted in optimal performance across all parameters and was clearly
distinct from all other parameter combinations (no 95% CI overlap).

```{r pbulk-cexpr-acc-plt, fig.cap = plot_caps$pbulk_cexpr_acc}
include_graphics(plot_paths$pbulk_cexpr_acc)
```

```{r pbulk-cexpr-acc-tab}
plot_data_paths$pbulk_cexpr_acc %>%
  fread(sep = ",") %>%
  mutate(
    `95% CI` = paste("\U00B1", round(mean_correlation - corr_min_ci, 3))
  ) %>%
  select(
    reference.metric, reference.split_cancer,
    mean_correlation, `95% CI`, reference.threshold, n_transcripts_marker_mean
  ) %>%
  kbl(
    caption = table_caps$pbulk_cexpr_acc,
    escape = FALSE,
    col.names = c(
      "Marker method", "Reference type",
      # For escape FALSE, `%` nedds to be escaped.
      "Avg. $\\rho$", "95\\% CI", "Threshold", "n marker genes"
    )
  ) %>%
  collapse_rows(
    columns = 1:2, valign = "top", row_group_label_position = "first"
  ) %>%
  kable_minimal()
```

## Evaluation

### Comparison to BayesPrism

When comparing my method to BayesPrism on both pseudo-bulk and bulk RNA-seq
data, both showed similar behavior (Figure \@ref(fig:bp-cexpr-acc-plt), Table
\@ref(tab:bp-cexpr-acc-plt-tab)).

On pseudo-bulks derived from distinct tumor samples, my method showed superior
correlation between true and estimated cancer expression for the tightest
threshold on marker metrics, but for any looser thresholds no clear distinctions
in performance can be made. BayesPrism showed a general trend of better
performance for looser marker thresholds, but few differences were statistically
significant (non-overlapping 95% CIs). Here, CIs also became wider for smaller,
more restrictive thresholds.

When applied to real-world bulk RNA-seq data, both my method and BayesPrism
showed significantly worse performance on pairs of otherwise equal parameters
(no pairwise 95% CI overlap). The trend of better correlation with looser
thresholds that BayesPrism showed for pseudo-bulks was also present here, and my
methods followed it as well. Lastly, in addition to the general worse
performance in bulk RNA-seq data, my method also showed a general pattern of
slightly lower performance compared to BayesPrism on this type of data (only
significant for the second-tightest and loosest threshold, 95% CIs).

```{r bp-cexpr-acc-plt, fig.cap = plot_caps$bp_cexpr_acc}
include_graphics(plot_paths$bp_cexpr_acc)
```

```{r bp-cexpr-acc-plt-tab}
plot_data_paths$bp_cexpr_acc %>%
  fread(sep = ",") %>%
  mutate(
    `95% CI` = paste("\U00B1", round(mean_correlation - corr_min_ci, 3))
  ) %>%
  select(-starts_with("corr")) %>%
  select(
    bulk_type, deconvolution.deconvolution_method,
    mean_correlation, `95% CI`, reference.threshold, n_transcripts_marker_mean
  ) %>%
  arrange(bulk_type, deconvolution.deconvolution_method) %>%
  kbl(
    caption = table_caps$bp_cexpr_acc,
    escape = FALSE,
    col.names = c(
      # For escape FALSE, `%` nedds to be escaped.
      "Type", "Method", "Avg. $\\rho$", "95\\% CI", "Threshold", "n marker genes"
    )
  ) %>%
  collapse_rows(
    columns = 1:2, valign = "top", row_group_label_position = "first"
  ) %>%
  kable_minimal()
```

### Prediction

Neither models using data generated from my method, nor from BayesPrism were
able to predict either the tumor intrinsic property or the patient level
clinical outcome better than all the models trained on raw bulk data.

When predicting the PAM50 subtype, the optimal model with the highest average
accuracy was the one trained on raw bulk data using the same set of genes as all
models trained with data from my method (Figure \@ref(fig:pred-cat-perf-plt),
Table \@ref(tab:pred-cat-perf-tab)). Models trained on cancer expression
estimates from my method performed relatively similar, with a looser threshold
on marker gene scores corresponding to marginally better model performance. For
models trained on estimates from BayesPrism, there was a considerably wider
spread in average accuracy. Here too, looser thresholds on the marker genes
resulted in better performance, but the tightest set of marker genes resulted in
remarkably low accuracy. The optimal gene set for BayesPrism still performed as
well as the optimum of my method. Models trained on raw bulk data generally
followed the trend of improving performance with larger gene sets, only the
model trained on bulk data retaining all its genes had a lower accuracy than
models trained using smaller gene sets. Except for the second-loosest threshold
on BayesPrism, no model trained on estimated expression data was able to
outperform a model trained on raw bulk data using the same set of genes.

The picture of the performance of models predicting a patients PFI was not as
clear as for the prediction of the PAM50 subtype (Figure
\@ref(fig:pred-srv-perf-plt), Table \@ref(tab:pred-srv-perf-tab)). While my method
maintained its general ordering in performance, the differences between the
loosest and second-loosest threshold, and between the tightest and the raw bulk
data with the same gene set was no longer clean (95% CI overlap). For
BayesPrism, models trained on its cancer expression estimates no longer
conformed to the same pattern. There was no clear difference in average C-index
between the loosest and tightest threshold, with both being slightly
outperformed by the intermediate threshold. The performance of models trained on
raw bulk data did also not follow a clear pattern across the size of the gene
set used for prediction. For the smallest set of genes, raw bulk model
performance was among the lowest overall, but the next larger gene set resulted
in the overall optimal performance. After this peak, a larger gene set only led
to worse performance, with the gene set matching the one used for my method, and
the full gene set both resulting in the worst performance across all models.

```{r pred-cat-perf-plt, fig.cap = plot_caps$pred_cat_perf}
include_graphics(plot_paths$pred_cat_perf)
```

```{r pred-cat-perf-tab}
# FIXME Combine pred tables & plots.
with_options(
  list(knitr.kable.NA = ""),
  plot_data_paths$pred_cat_perf %>%
    fread(sep = ",") %>%
    mutate(
      `95% CI` = paste("\U00B1", round(mean_accuracy - min_ci, 3))
    ) %>%
    mutate(
      # To avoid confusion, remove deconv method for downsampled raw bulk.
      deconv_method = ifelse(extract_type == "Raw bulk", NA, deconv_method)
    ) %>%
    select(-c(min_ci, max_ci)) %>%
    select(
      "extract_type", "deconv_method", "mean_accuracy", "95% CI",
      "n_transcript", "threshold", "n_marker_transcripts"
    ) %>%
    kbl(
      caption = table_caps$pred_cat_perf,
      escape = FALSE,
      col.names = c(
        "Data type", "Method",
        # For escape FALSE, `%` nedds to be escaped.
        "Avg. bootstrapped accuracy", "95\\% CI", "n genes", "Threshold",
        "n marker genes"
      )
    ) %>%
    kable_minimal() %>%
    landscape()
)
```

```{r pred-srv-perf-plt, fig.cap = plot_caps$pred_srv_perf}
include_graphics(plot_paths$pred_srv_perf)
```

```{r pred-srv-perf-tab}
with_options(
  list(knitr.kable.NA = ""),
  plot_data_paths$pred_srv_perf %>%
    fread(sep = ",") %>%
    mutate(
      `95% CI` = paste("\U00B1", round(mean_c_index - min_ci, 3))
    ) %>%
    mutate(
      # To avoid confusion, remove deconv method for downsampled raw bulk.
      deconv_method = ifelse(extract_type == "Raw bulk", NA, deconv_method)
    ) %>%
    select(-c(min_ci, max_ci)) %>%
    select(
      "extract_type", "deconv_method", "mean_c_index", "95% CI",
      "n_transcript", "threshold", "n_marker_transcripts"
    ) %>%
    kbl(
      caption = table_caps$pred_srv_perf,
      escape = FALSE,
      col.names = c(
        "Data type", "Method",
        # For escape FALSE, `%` nedds to be escaped.
        "Avg. bootstrapped C index", "95\\% CI", "n genes", "Treshold",
        "n marker genes"
      )
    ) %>%
    kable_minimal() %>%
    landscape()
)
```
