# Results

```{r res-setup, include = FALSE}
import::here("here", "here")
import::here("knitr", "include_graphics")

# Suppress chunk output by default.
knitr::opts_chunk$set(echo = FALSE)

here::i_am("bookdown/03-results.Rmd")

import::here(
  "utils.R",
  "main_plot_handles_from_path",
  .character_only = TRUE,
  .directory = here("modules")
)

analysis_root <- "cancer-cleaning-output"
main_plot_dir <- "main_plots"
main_plot_path <- here(paste(analysis_root, main_plot_dir, sep = "/"))

plot_prefixes <- list(
  "rnd_sim" = "random_deconv_simulation_metric_summary_plot",
  "pbulk_cexpr_acc" = "simulation_true_v_predict_cancer_corr_plot",
  "bp_cexpr_acc" = "bayes_prism_true_v_predict_cancer_corr_plot",
  "pred_cat_perf" = "categorical_prediction_performance_plot",
  "pred_srv_perf" = "survival_prediction_performance_plot"
)

plot_handles <- main_plot_handles_from_path(main_plot_path, plot_prefixes)

plot_paths <- plot_handles$plot_paths
plot_caps <- plot_handles$plot_caps
```

## Proof of concept

### Normally distributed simulation

<!-- TODO Rephrase this as groups having more technical noise. -->

<!-- TODO Go into more detail of the behavior, especially pairwise cell type
expression correlation. -->

During simulation using normally distributed data, deconvPure's performance
behaved as expected. The estimation accuracy for cancer expression via
deconvolution model residuals (Figure \@ref(fig:rnd-sim-plt), Table
\@ref(tab:rnd-sim-res-tab)) shows clear differences across simulation
parameters, with the largest difference being across reference type.

The average correlation of true and estimated cancer expression under the
homogeneous cancer expression model using a reference with cancer profiles was
close to zero. However, there were still differences between relative levels of
added noise: Absent noise and noise equal to biological variability had the same
levels of accuracy, while the high noise group was slightly worse, and the group
with very high biological noise had exactly zero correlation. Under the model of
cancer cells varying stronger than other cell type's cells, and still including
cancer profiles in the reference, there was an appreciable improvement in
accuracy compared to the homogeneous model. The performance across groups
remained similar to the previous model. As mentioned, the largest improvement
can be seen when removing the cancer expression profile from the reference.
Here, correlation of true and estimated cancer expression is high across almost
all levels of technical noise except for the highest. This extends across cancer
expression models, with the heterogeneous model still showing slightly higher
correlations. Under these optimal conditions, the average correlation ranged
between around 0.9 for absent and equal technial noise.

```{r rnd-sim-plt, fig.cap = plot_caps$rnd_sim}
include_graphics(plot_paths$rnd_sim)
```

### scRNA-seq pseudo-bulks

When benchmarking deconvPure on pseudo-bulks derived from scRNA-seq data, the
accuracy of estimated cancer expression from absolute values of model residuals
was high (Figure \@ref(fig:pbulk-cexpr-acc-plt), Table
\@ref(tab:pbulk-cexpr-acc-tab)) with only minor differences in average
correlation across parameters.

During simulation and deconvolution I observed that for many samples there
existed an association between true cancer expression and overall bulk
expression (Supp. Figure \@ref(fig:pbulk-sim-diag-1) (C) & Supp. Figure
\@ref(fig:pbulk-sim-diag-2) (C)). Indeed, for almost all of the cell types (
save for Plasmablasts and to a lesser extent for Myeloid cells), there existed
strong colinearity in expression profiles between each cell type and the others
in the sample (Supp. Figure \@ref(fig:pbulk-sim-diag-1) (D) & Supp. Figure
\@ref(fig:pbulk-sim-diag-2) (D)).

When including cancer expression in the reference profiles, there was a clear
difference between marker selection methods. Wilcoxon test based markers showed
higher average correlation compared to random marker selection across all
thresholds on the number of markers (no 95% CI overlap). Also, while there was
very little variation in accuracy across thresholds for random markers, on the
second-tightest threshold for Wilcoxon based markers performance was
significantly worse than the on the remaining thresholds. When excluding the
cancer expression profile from the reference, performance further increased
across all other parameters (no pairwise 95% CI overlap), except for the
tightest marker threshold on Wilcoxon.

General patterns were similar between in- or excluding the cancer expression
profile but with some exceptions. In the latter case and for the loosest marker
threshold, random markers resulted in slightly but significantly worse average
correlation than looser thresholds, which in turn no longer clearly differed
from the second-tightest threshold on Wilcoxon markers. Lastly, the within group
differences for Wilcoxon markers increased such that the two loosest thresholds
no longer resulted in similar performance. The second-loosest marker threshold
now resulted in optimal performance across all parameters and was clearly
distinct from all other parameter combinations (no 95% CI overlap).

<!-- TODO Mention approx. point of change. -->

Lastly, I observed a relationship between the proportion of cancer cells in a
sample and the accuracy of expression estimation (Supp. Figure
\@ref(fig:pbulk-acc-v-abund)). For samples with very small amounts of cancer
cells (i.e., low tumor purity), the estimation accuracy was very low, but rose
sharply with increasing tumor purity. For purities higher than about XXX, the
increase in accuracy slowed, and became close to linear up to maximum purity,
indicating a roughly logarithmic relationship. This behavior was constant across
paramameter combinations.

```{r pbulk-cexpr-acc-plt, fig.cap = plot_caps$pbulk_cexpr_acc}
include_graphics(plot_paths$pbulk_cexpr_acc)
```

## Evaluation

### Comparison to BayesPrism

When comparing deconvPure to BayesPrism on both pseudo-bulk and bulk RNA-seq
data, both showed similar behavior (Figure \@ref(fig:bp-cexpr-acc-plt), Table
\@ref(tab:bp-cexpr-acc-plt-tab)).

On pseudo-bulks derived from distinct tumor samples, deconvPure showed superior
correlation between true and estimated cancer expression for the tightest
threshold on marker metrics, but for any looser thresholds no clear distinctions
in performance can be made. BayesPrism showed a general trend of better
performance for looser marker thresholds, but few differences were statistically
significant (non-overlapping 95% CIs). Here, CIs also became wider for smaller,
more restrictive thresholds.

When applied to real-world bulk RNA-seq data, both deconvPure and BayesPrism
showed significantly worse performance on pairs of otherwise equal parameters
(no pairwise 95% CI overlap). The trend of better correlation with looser
thresholds that BayesPrism showed for pseudo-bulks was also present here, and my
methods followed it as well. Lastly, in addition to the general worse
performance in bulk RNA-seq data, deconvPure also showed a general pattern of
slightly lower performance compared to BayesPrism on this type of data (only
significant for the second-tightest and loosest threshold, 95% CIs).

```{r bp-cexpr-acc-plt, fig.cap = plot_caps$bp_cexpr_acc}
include_graphics(plot_paths$bp_cexpr_acc)
```

### Prediction

Neither models using data generated from deconvPure, nor from BayesPrism were
able to predict either the tumor intrinsic property or the patient level
clinical outcome better than all the models trained on raw bulk data.

When predicting the PAM50 subtype, the optimal model with the highest average
accuracy was the one trained on raw bulk data using the same set of genes as all
models trained with data from deconvPure (Figure \@ref(fig:pred-cat-perf-plt),
Table \@ref(tab:pred-cat-perf-tab)). Models trained on cancer expression
estimates from deconvPure performed relatively similar, with a looser threshold
on marker gene scores corresponding to marginally better model performance. For
models trained on estimates from BayesPrism, there was a considerably wider
spread in average accuracy. Here too, looser thresholds on the marker genes
resulted in better performance, but the tightest set of marker genes resulted in
remarkably low accuracy. The optimal gene set for BayesPrism still performed as
well as the optimum of deconvPure. Models trained on raw bulk data generally
followed the trend of improving performance with larger gene sets, only the
model trained on bulk data retaining all its genes had a lower accuracy than
models trained using smaller gene sets. Except for the second-loosest threshold
on BayesPrism, no model trained on estimated expression data was able to
outperform a model trained on raw bulk data using the same set of genes.

The picture of the performance of models predicting a patients PFI was not as
clear as for the prediction of the PAM50 subtype (Figure
\@ref(fig:pred-srv-perf-plt), Table \@ref(tab:pred-srv-perf-tab)). Here, some of
the models trained on deconvPure were able to predict PFI with higher accuracy
than the model trained on raw bulk data with the same gene set, but not better
than models trained on raw bulk data with other gene sets. While deconvPure
maintained its general ordering in performance, the differences between the
models was no longer statistically significant in some cases (95% CI overlap).
For BayesPrism, models trained on its cancer expression estimates no longer
conformed to the same pattern. There was no clear difference in average C-index
between the loosest and tightest threshold, with both being slightly
outperformed by the intermediate threshold. The performance of models trained on
raw bulk data did also not follow a clear pattern across the size of the gene
set used for prediction. For the smallest set of genes, raw bulk model
performance was among the lowest overall, but the next larger gene set resulted
in the overall optimal performance. After this peak, a larger gene set only led
to worse performance, with the gene set matching the one used for deconvPure, and
the full gene set both resulting in the worst performance across all models.

```{r pred-cat-perf-plt, fig.cap = plot_caps$pred_cat_perf}
include_graphics(plot_paths$pred_cat_perf)
```

```{r pred-srv-perf-plt, fig.cap = plot_caps$pred_srv_perf}
include_graphics(plot_paths$pred_srv_perf)
```
