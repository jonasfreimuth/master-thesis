# Results

```{r res-setup, include = FALSE}
import::here("data.table", "fread", "uniqueN")
import::here("dplyr", "arrange", "mutate", "recode", "rename", "select")
import::here("here", "here")
import::here("knitr", "include_graphics")
import::here(
  "kableExtra",
  "add_header_above",
  "collapse_rows",
  "kable_minimal",
  "kable_styling",
  "kbl",
  "landscape"
)
import::here("magrittr", "%>%", "extract", "multiply_by", "set_names")
import::here("purrr", "map", "reduce")
import::here("tidyr", "pivot_wider")
import::here("withr", "with_options")

# Suppress chunk output by default.
knitr::opts_chunk$set(echo = FALSE)

here::i_am("bookdown/03-results.Rmd")

analysis_root <- "cancer-cleaning-output"
main_plot_dir <- "main_plots"
main_plot_path <- here(paste(analysis_root, main_plot_dir, sep = "/"))

plot_prefixes <- list(
  "rnd_sim" = "random_deconv_simulation_metric_summary_plot",
  "pbulk_cexpr_acc" = "simulation_true_v_predict_cancer_corr_plot",
  "bp_cexpr_acc" = "bayes_prism_true_v_predict_cancer_corr_plot",
  "pred_cat_perf" = "categorical_prediction_performance_plot",
  "pred_srv_perf" = "survival_prediction_performance_plot"
)

plot_paths <- map(
  plot_prefixes,
  \(prefix) paste(main_plot_path, paste0(prefix, ".png"), sep = "/")
)

plot_caps <- map(
  plot_prefixes,
  \(prefix) paste(main_plot_path, paste0(prefix, "_caption.txt"), sep = "/")
) %>%
  map(readLines)

plot_data_paths <- map(
  plot_prefixes,
  \(prefix) paste(main_plot_path, paste0(prefix, ".csv"), sep = "/")
)

# TODO Add table captions.
```

## Proof of concept

### Normally distributed simulation

During simulation using normally distributed data, my method's performance
behaved as expected.

<!-- TODO Rephrase this as groups having more technical noise. -->

<!-- TODO Go into more detail of the behavior, especially pairwise cell type
expression correlation. -->

The accuracy of deconvolution model residuals as an estimate of cancer
expression on the other hand shows an almost complementary picture (Figure
\@ref(fig:rnd-sim-plt)). Here, the correlation under the homogeneous cancer
expression and including cancer profiles in the reference was very poor.
However, there were still differences between the groups: High & equal
biological noise had the same levels of accuracy, while the low biological noise
group was slightly worse, and the very low biological noise showed levels of
accuracy barely distinguishable from zero. Under the model of cancer cells
varying stronger than other cell type's cells, and still including cancer
profiles in the reference, there was an appreciable improvement in accuracy
compared to the homogeneous model. The performance across groups remained
similar to the previous model. The best improvement can be seen however when
removing the cancer expression profile from the reference. Cancer expression
accuracy here is very good across levels of biological noise except the lowest,
and regardless of cancer expression model.

```{r rnd-sim-plt, fig.cap = plot_caps$rnd_sim}
include_graphics(plot_paths$rnd_sim)
```

```{r rnd-sim-res-tab}
rnd_sim_data <- plot_data_paths$rnd_sim %>%
  fread(sep = ",") %>%
  mutate(across(
    where(is.character),
    \(char_col) factor(char_col, levels = unique(char_col))
  ))

# Recode types so they appear in order when sorting cols after pivot_longer.
type_recode <- list(
  # Currently unused.
  "True vs. predicted Cell type abundance 1 / RMSE" = "a ctype_abundances",
  "Cancer expression vs. Model residuals R²" = "b cancer_accuracy"
)

# Depends on the same ordering as type_recode.
type_headers <- c(
  # Currently unused.
  "True vs. predicted Cell type abundance 1 / RMSE" = "Cell type accurracy, $1 / RMSE$",
  "Cancer expression vs. Model residuals R²" = "Cancer expression accurracy $\\rho$"
) %>%
  extract(names(.) %in% unique(rnd_sim_data$type)) %>%
  # Create a list with previous values as names and `2` (fixed width, mean & ci)
  # as values.
  {
    set_names(rep(list(2), length(.)), .)
  }

rnd_sim_data %>%
  # Maybe the cols removed here will be removed somewhere upriver already. If
  # that's the case, this can be removed.
  select(-any_of(c("n", "n_rmse"))) %>%
  mutate(
    `95% CI` = paste("\U00B1", round(mean - min_ci, 3)),
    type = recode(type, !!!type_recode)
  ) %>%
  select(-matches("_ci")) %>%
  pivot_wider(
    values_from = c("mean", "95% CI"),
    names_from = "type",
    names_glue = "{type} {.value}"
  ) %>%
  select(
    heterogeneous_cancer,
    split,
    bio_tech_ratio,
    # Should in ctype acc first and mean first
    all_of(sort(names(.), decr = TRUE))
  ) %>%
  arrange(heterogeneous_cancer, split, bio_tech_ratio) %>%
  kbl(
    caption = "TODO ADD PROPER TABLE CAPTIONS",
    escape = TRUE,
    col.names = c(
      "Cancer model",
      "Reference type",
      "Relative noise level name",
      rep(c("Mean", "95% CI"), uniqueN(rnd_sim_data$type))
    ),
    align = "l"
  ) %>%
  kable_minimal() %>%
  add_header_above(c(
    " " = 3,
    type_headers
  )) %>%
  collapse_rows(columns = 1:3, valign = "top") %>%
  landscape()
```

### scRNA-seq pseudo-bulks

When benchmarking my method on pseudo-bulks derived from scRNA-seq data, the
performance was not as consistent as during the simulation using normally
distributed data. For all parameter combinations, the accuracy of estimated
cancer expression from absolute values of model residuals was high
(\@ref(fig:pbulk-cexpr-acc-plt)), with only minor differences.

When including cancer expression in the reference profiles, there was a clear
difference between marker selection methods. Where Wilcoxon test based markers
showed improved performance compared to random marker selection across all
thresholds on the number of markers (significant at $\alpha = 0.05$, 95%
confidence intervals). Also, while there was very little variation in accuracy
across thresholds for random markers, on the second-tightest threshold for
Wilcoxon based markers performance was significantly worse than the on the
remaining thresholds. When excluding the cancer expression profile from the
reference, performance further increased across all other parameters
(significant at $\alpha = 0.05$, 95% confidence intervals), except for the
tightest marker threshold on Wilcoxon.

General patterns were similar between in- or excluding the cancer expression
profile but with some exceptions. In the latter case and for the loosest marker
threshold, random markers resulted in slightly but significantly worse
performance than looser thresholds, who in turn no longer clearly differed from
the second-tightest threshold on Wilcoxon markers. Lastly, the within group
differences for Wilcoxon markers increased such that the two loosest thresholds
no longer resulted in similar performance. The second-loosest marker threshold
now resulted in optimal performance across all parameters and was clearly
distinct from all other parameter combinations.

```{r pbulk-cexpr-acc-plt, fig.cap = plot_caps$pbulk_cexpr_acc}
include_graphics(plot_paths$pbulk_cexpr_acc)
```

```{r pbulk-cexpr-acc-tab}
plot_data_paths$pbulk_cexpr_acc %>%
  fread(sep = ",") %>%
  mutate(
    `95% CI` = paste("\U00B1", round(mean_correlation - corr_min_ci, 3)),
    # FIXME Ensure ordering happens in main_plots.
    reference.metric = factor(
      reference.metric, levels = c("Random", "Wilcoxon")
    ),
    reference.split_cancer = factor(
      reference.split_cancer, levels = c(
        "Excl. cancer profile", "Incl. cancer profile"
      )
    )
  ) %>%
  select(
    reference.metric, reference.split_cancer,
    mean_correlation, `95% CI`, n_transcripts_marker_mean
  ) %>%
  arrange(reference.metric, reference.split_cancer) %>%
  kbl(
    caption = "TODO ADD PROPER TABLE CAPTIONS",
    escape = FALSE,
    col.names = c(
      "Marker method", "Reference type",
      # For escape FALSE, `%` nedds to be escaped.
      "Avg. $\\rho$", "95\\% CI", "n marker genes"
    )
  ) %>%
  collapse_rows(
    columns = 1:2, valign = "top", row_group_label_position = "first"
  ) %>%
  kable_minimal()
```

## Evaluation

### Comparison to BayesPrism

In comparison to BayesPrism, my method largely had a similar performance (Figure
\@ref(fig:bp-cexpr-acc-plt)). On pseudo-bulks derived from distinct tumor
samples, my method showed superior correlation performance for the tightest
threshold on marker metrics, but for any looser thresholds, no clear
distinctions in performance can be made. BayesPrism showed a general trend of
better performance for looser marker thresholds, but few differences were
statistically significant (non-overlapping 95% confidence intervals). Here,
confidence intervals also became wider for smaller, more restrictive thresholds.

When applied to real-world bulk RNA-seq data, both my method and BayesPrism
showed significantly worse performance on pairs of otherwise equal parameters.
Here, my method also showed a general pattern of worse performance (although
only significant for the second-tightest and loosest threshold, 95% confidence
intervals).

```{r bp-cexpr-acc-plt, fig.cap = plot_caps$bp_cexpr_acc}
include_graphics(plot_paths$bp_cexpr_acc)
```

```{r bp-cexpr-acc-plt-tab}
plot_data_paths$bp_cexpr_acc %>%
  fread(sep = ",") %>%
  mutate(
    `95% CI` = paste("\U00B1", round(mean_correlation - corr_min_ci, 3))
  ) %>%
  select(-starts_with("corr")) %>%
  select(
    bulk_type, deconvolution.deconvolution_method,
    mean_correlation, `95% CI`, n_transcripts_marker_mean
  ) %>%
  arrange(bulk_type, deconvolution.deconvolution_method) %>%
  kbl(
    caption = "TODO ADD PROPER TABLE CAPTIONS",
    escape = TRUE,
    col.names = c(
      "Type", "Method", "Avg. $\\rho$", "95% CI", "n marker genes"
    )
  ) %>%
  kable_minimal()
```

### Prediction

Neither models using data generated from my method, nor from BayesPrism were
able to predict either the tumor intrinsic property or the patient level
clinical outcome better than all the models trained on raw bulk data.

When predicting the PAM50 subtype, the optimal model was the one trained on raw
bulk data using the same set of genes as all models trained with data from my
method (Figure \@ref(fig:pred-cat-perf-plt)). Models trained on cancer
expression estimates from my method performed relatively similar, with a looser
threshold on marker gene scores corresponding to marginally better model
performance. For models trained on estimates from BayesPrism, there was a
considerably wider spread in accuracy. Here too, looser thresholds on the marker
genes resulted in better performance, but the tightest set of marker genes
resulted in remarkably low accuracy. The optimal gene set for BayesPrism still
performed as well as the optimum of my method. Models trained on raw bulk data
generally followed the trend of improving performance with larger gene sets,
only the model trained on bulk data retaining all its genes had a lower accuracy
than models trained using smaller gene sets. Except for the second-loosest
threshold on BayesPrism, no model trained on estimated expression data was able
to outperform a model trained on raw bulk data using the same set of genes.

The picture of the performance of models predicting a patients PFI was not as
clear as for the prediction of the PAM50 subtype (Figure
\@ref(fig:pred-srv-perf-plt)). While my method maintained its general ordering
in performance, the differences between the loosest and second-loosest
threshold, and between the tightest and the raw bulk data with the same gene set
was no longer clean (95% CI overlap). For BayesPrism, models trained on its
cancer expression estimates no longer conformed to the same pattern. There was
no clear difference between the loosest and tightest threshold, with both being
slightly outperformed by the intermediate threshold. The performance of models
trained on raw bulk data did also not follow a clear pattern across the size of
the gene set used for prediction. For the smallest set of genes, raw bulk model
performance was among the lowest overall, but the next larger gene set resulted
in the overall optimal performance. After this peak, a larger gene set only led
to worse performance, with the gene set matching the one used for my method, and
the full gene set both resulting in the worst performance across all models.

```{r pred-cat-perf-plt, fig.cap = plot_caps$pred_cat_perf}
include_graphics(plot_paths$pred_cat_perf)
```

```{r pred-cat-perf-tab}
# FIXME Combine pred tables & plots.
with_options(
  list(knitr.kable.NA = ""),
  plot_data_paths$pred_cat_perf %>%
    fread(sep = ",") %>%
    mutate(
      `95% CI` = paste("\U00B1", round(mean_accuracy - min_ci, 3))
    ) %>%
    mutate(
      # To avoid confusion, remove deconv method for downsampled raw bulk.
      deconv_method = ifelse(extract_type == "Raw bulk", NA, deconv_method)
    ) %>%
    select(-c(min_ci, max_ci)) %>%
    select(
      "extract_type", "deconv_method", "mean_accuracy", "95% CI",
      "n_transcript", "n_marker_transcripts"
    ) %>%
    kbl(
      caption = "TODO ADD PROPER TABLE CAPTIONS",
      escape = TRUE,
      col.names = c(
        "Data type", "Method",
        "Avg. bootstrapped accuracy", "95% CI", "n genes", "n marker genes"
      )
    ) %>%
    kable_minimal()
)
```

```{r pred-srv-perf-plt, fig.cap = plot_caps$pred_srv_perf}
include_graphics(plot_paths$pred_srv_perf)
```

```{r pred-srv-perf-tab}
with_options(
  list(knitr.kable.NA = ""),
  plot_data_paths$pred_srv_perf %>%
    fread(sep = ",") %>%
    mutate(
      `95% CI` = paste("\U00B1", round(mean_c_index - min_ci, 3))
    ) %>%
    mutate(
      # To avoid confusion, remove deconv method for downsampled raw bulk.
      deconv_method = ifelse(extract_type == "Raw bulk", NA, deconv_method)
    ) %>%
    select(-c(min_ci, max_ci)) %>%
    select(
      "extract_type", "deconv_method", "mean_c_index", "95% CI",
      "n_transcript", "n_marker_transcripts"
    ) %>%
    kbl(
      caption = "TODO ADD PROPER TABLE CAPTIONS",
      escape = TRUE,
      col.names = c(
        "Data type", "Method",
        "Avg. bootstrapped C index", "95% CI", "n genes", "n marker genes"
      )
    ) %>%
    kable_minimal()
)
```
